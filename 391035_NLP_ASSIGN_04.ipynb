{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Name: Mahesh Marathe**\n",
        "  \n",
        "\n",
        "  \n",
        "  **PRN-22211533**\n",
        "  \n",
        "  **Roll_No.: 391035**\n",
        "\n",
        "**Batch-A2**"
      ],
      "metadata": {
        "id": "XSAidjJBzEN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title:-Create a Transformer using Pytorch Library"
      ],
      "metadata": {
        "id": "aMGLI4DFzc9n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objectives:**\n",
        "\n",
        "1.To understand the architecture and functioning of the Transformer model in deep learning.\n",
        "\n",
        "2.To implement a custom Transformer architecture using PyTorch.\n",
        "\n",
        "3.To learn the practical usage of attention mechanisms, especially self-attention.\n",
        "\n",
        "4.To explore the role of Transformer models in modern NLP tasks like machine translation, summarization, and question answering.\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "The Transformer model, introduced in the paper “Attention is All You Need” (Vaswani et al., 2017), is a deep learning architecture based purely on attention mechanisms, removing recurrence entirely. It has become the foundation for models like BERT, GPT, and T5.\n",
        "\n",
        "Key Components of Transformer:\n",
        "Input Embedding + Positional Encoding:\n",
        "Since Transformers don't have recurrence, positional encodings are added to input embeddings to provide token order information.\n",
        "\n",
        "Multi-Head Self Attention:\n",
        "Allows the model to focus on different positions of the sequence for each token, enhancing context understanding.\n",
        "\n",
        "Feed Forward Neural Network:\n",
        "Each position's attention output is passed through a fully connected network.\n",
        "\n",
        "Add & Norm:\n",
        "Layer normalization and residual connections ensure stability and better gradient flow.\n",
        "\n",
        "Encoder & Decoder Blocks:\n",
        "\n",
        "Encoder: Processes the input sequence.\n",
        "\n",
        "Decoder: Generates the output sequence using encoder outputs and self-attention.\n",
        "\n",
        "Applications:\n",
        "Machine Translation\n",
        "\n",
        "Text Summarization\n",
        "\n",
        "Chatbots\n",
        "\n",
        "Speech Recognition\n",
        "\n",
        "Text Generation\n"
      ],
      "metadata": {
        "id": "4rmS2_tKziMg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRRfMXEpxgq6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Transformer\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bB7JZIzxt06",
        "outputId": "c566d0d4-d91e-4f4e-9689-6a08d4ade70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ecbcc5da630>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 1000\n",
        "tgt_vocab_size = 1000\n",
        "embedding_size = 512\n",
        "num_heads = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.1\n",
        "src_seq_length = 10\n",
        "tgt_seq_length = 10\n",
        "batch_size = 32\n"
      ],
      "metadata": {
        "id": "AWxuSyScx0d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size, maxlen=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(maxlen, emb_size)\n",
        "        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ],
      "metadata": {
        "id": "kbbTe-Scx4SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, src_vocab, tgt_vocab, emb_size, nhead, num_encoder_layers, num_decoder_layers, dropout):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab, emb_size)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab, emb_size)\n",
        "        self.pos_encoder = PositionalEncoding(emb_size)\n",
        "        self.pos_decoder = PositionalEncoding(emb_size)\n",
        "        self.transformer = Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(emb_size, tgt_vocab)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.pos_encoder(self.src_embedding(src))\n",
        "        tgt_emb = self.pos_decoder(self.tgt_embedding(tgt))\n",
        "        output = self.transformer(src_emb.permute(1, 0, 2), tgt_emb.permute(1, 0, 2))\n",
        "        output = self.fc_out(output.permute(1, 0, 2))\n",
        "        return output"
      ],
      "metadata": {
        "id": "QXIwu-_Yx9GJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerModel(src_vocab=src_vocab_size, tgt_vocab=tgt_vocab_size,\n",
        "                         emb_size=embedding_size, nhead=num_heads,\n",
        "                         num_encoder_layers=num_encoder_layers,\n",
        "                         num_decoder_layers=num_decoder_layers,\n",
        "                         dropout=dropout)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF1WSiuAyDr9",
        "outputId": "8467c532-483f-4f4a-fdb1-02f451a37466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_length))\n",
        "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_length))"
      ],
      "metadata": {
        "id": "oKVZ471fyL4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(src, tgt)\n",
        "print(\"Transformer output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DeCE4hCyQYN",
        "outputId": "8cccbf00-0e8b-415c-dc7f-93987d2afc0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer output shape: torch.Size([32, 10, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "This assignment demonstrates the construction of a basic Transformer model using PyTorch. By creating each component from scratch, we gain a deeper understanding of how attention mechanisms power modern deep learning models. Transformers continue to revolutionize NLP and have applications in audio, vision, and multi-modal AI systems."
      ],
      "metadata": {
        "id": "FCwH9aGzztCv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AUG5Ch4byTD0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}