{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ASSIGNMENT - 1\n",
        "Name:- Mahesh Marathe   RollNo. :- 391035  PRN:- 22211533  Batch:- A2"
      ],
      "metadata": {
        "id": "IwPwgZewHp0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Title:-Perform tokenization (Whitespace, Punctuation-based, Treebank, Tweet, MWE) using NLTK library. Use porter stemmer and snowball stemmer for stemming. Use any technique for lemmatization.\n",
        "\n",
        "\n",
        "Objectives:\n",
        "To understand how tokenization helps break down text into manageable pieces.\n",
        "\n",
        "To apply different types of tokenization such as whitespace, punctuation-based, Treebank, Tweet, and Multi-Word Expression (MWE) using the NLTK library.\n",
        "\n",
        "To explore stemming techniques using Porter Stemmer and Snowball Stemmer.\n",
        "\n",
        "To apply lemmatization using an appropriate technique.\n",
        "\n",
        "To understand the importance of these preprocessing techniques in Natural Language Processing (NLP) pipelines.\n",
        "\n",
        "\n",
        "Theory:\n",
        "1. Tokenization:\n",
        "Tokenization is the process of splitting raw text into smaller parts called tokens. These tokens can be words, phrases, or symbols. It is usually the first step in any NLP task.\n",
        "\n",
        "Types of Tokenizers:\n",
        "\n",
        "Whitespace Tokenizer: Splits text based on spaces only. It is simple but does not handle punctuation or special characters well.\n",
        "\n",
        "Punctuation-based Tokenizer: Splits text based on spaces and punctuation marks. Slightly more advanced than whitespace tokenization.\n",
        "\n",
        "Treebank Tokenizer: Provided by the Penn Treebank corpus. It handles complex text structures like contractions and punctuation more intelligently.\n",
        "\n",
        "Tweet Tokenizer: Designed specifically for tokenizing social media text. It handles hashtags, mentions, emojis, and repeated characters efficiently.\n",
        "\n",
        "MWE Tokenizer (Multi-Word Expression Tokenizer): Groups together expressions like \"New York\" or \"Prime Minister\" as a single token rather than splitting them into individual words.\n",
        "\n",
        "2. Stemming:\n",
        "Stemming is the process of reducing a word to its root or base form, which may not necessarily be a valid word. It helps in normalizing the text by removing prefixes and suffixes.\n",
        "\n",
        "Porter Stemmer: One of the oldest stemming algorithms. It uses a rule-based approach to remove common morphological and inflectional endings.\n",
        "\n",
        "Snowball Stemmer: An improved version of Porter Stemmer. It is more aggressive and supports multiple languages. Also known as the English stemmer.\n",
        "\n",
        "Example:\n",
        "Words like \"running\", \"runner\", and \"ran\" may all be reduced to \"run\" using stemming.\n",
        "\n",
        "3. Lemmatization:\n",
        "Lemmatization is similar to stemming but more accurate. It reduces a word to its base or dictionary form (called a lemma) using linguistic knowledge.\n",
        "\n",
        "The most commonly used lemmatizer in NLTK is the WordNetLemmatizer. It uses the WordNet database to find the correct base form of the word.\n",
        "\n",
        "Unlike stemming, lemmatization ensures the result is a valid word. For example, \"better\" is lemmatized to \"good\", and \"running\" becomes \"run\".\n",
        "\n"
      ],
      "metadata": {
        "id": "IeirUahwIKmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM5ji_PWHs-R",
        "outputId": "a0e2170a-6c04-41dd-edac-460ec9f93f80"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNTYwTzlKJfV",
        "outputId": "5cf09a97-5d3a-481d-88da-ef3c0a162fca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original String : \n",
            " Welcome to AIML \n",
            " HELLO WORD\n",
            "\n",
            " Tokenized text : \n",
            "['Welcome', 'to', 'AIML', 'HELLO', 'WORD']\n"
          ]
        }
      ],
      "source": [
        "#Whitespace tokenization\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "#Creating a reference variable\n",
        "wt = WhitespaceTokenizer()\n",
        "\n",
        "text = \"Welcome to AIML \\n HELLO WORD\"\n",
        "print(\"Original String : \\n\", text)\n",
        "\n",
        "#tokenized method\n",
        "tokenized_text = wt.tokenize(text)\n",
        "print(\"\\n Tokenized text : \")\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31ixLcEMKJfW",
        "outputId": "43571c1e-45f6-49a7-edec-a21039822b38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Original String : \n",
            "HELLO World; I love working with NLP,1,2,3,4:\n",
            "\n",
            " Split all punctuations into separate tokens : \n",
            "['HELLO', 'World', ';', 'I', 'love', 'working', 'with', 'NLP', ',', '1', ',', '2', ',', '3', ',', '4', ':']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "text = \"HELLO World; I love working with NLP,1,2,3,4:\"\n",
        "print(\"\\n Original String : \")\n",
        "print(text)\n",
        "result = WordPunctTokenizer().tokenize(text)\n",
        "print(\"\\n Split all punctuations into separate tokens : \")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFhbsKrhKJfX",
        "outputId": "6a095ba7-dbf5-44bd-a70c-7e73f221876d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HELLO', 'World', ';', 'I', 'love', 'working', 'with', 'NLP,1,2,3,4', ':']\n"
          ]
        }
      ],
      "source": [
        "from nltk import TreebankWordTokenizer\n",
        "\n",
        "result = TreebankWordTokenizer().tokenize(text)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJjIT8SLKJfY",
        "outputId": "ad6b0acc-6175-4c11-e4d9-912a1d877bdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['âŒ›', 'SALE', 'UPTO', '85', '%', 'OR', 'MAYBE', 'âŒ›', 'UP', 'TO', '70', '%', 'OFF', 'ðŸ˜®']\n"
          ]
        }
      ],
      "source": [
        "from nltk import TweetTokenizer\n",
        "\n",
        "tweet_tokenize = TweetTokenizer()\n",
        "sample_tweet = \"âŒ›SALE UPTO 85% OR MAYBE âŒ› UP TO 70% OFF ðŸ˜®\"\n",
        "print(tweet_tokenize.tokenize(sample_tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leUZ2f8WKJfZ",
        "outputId": "be6a6c28-746e-4a76-a5cc-ade5686b5fb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In', 'a', 'litle', 'or', 'a_little_bit', 'or', 'a_lot', 'in_spite_of']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from nltk import MWETokenizer\n",
        "tokenizer = MWETokenizer([('a', 'little'), ('a', 'little', 'bit'), ('a', 'lot')])\n",
        "\n",
        "tokenizer.add_mwe(('in', 'spite', 'of'))\n",
        "tokenizer.tokenize('In a litle or a little bit or a lot in spite of '.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT97AePxKJfZ",
        "outputId": "6becfa7a-84a1-4b7b-85e2-33196af3effa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mahesh_Marathe', 'is', 'a', 'good', 'Student']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('Mahesh', 'Marathe'))\n",
        "tokenizer.tokenize('Mahesh Marathe is a good Student' .split())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "Tokenization, stemming, and lemmatization are crucial steps in preprocessing raw text data for natural language processing tasks. These steps help in reducing complexity, removing noise, and improving the performance of downstream tasks such as classification, sentiment analysis, and language modeling. The NLTK library provides powerful tools to apply these techniques efficiently in Python.\n",
        "\n"
      ],
      "metadata": {
        "id": "_iA4qODjI5np"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5l9TAFmII9Cp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}